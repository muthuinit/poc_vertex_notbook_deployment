{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10822876,"sourceType":"datasetVersion","datasetId":6719991}],"dockerImageVersionId":30918,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Importing libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.model_selection import GridSearchCV, KFold\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nfrom xgboost import XGBRegressor\n\n# Load dataset\ndf = pd.read_csv(\"/kaggle/input/housing-prices-dataset/Housing.csv\")\ndf.dropna(inplace=True)\n\n# Remove outliers using IQR\nQ1, Q3 = df[\"price\"].quantile([0.25, 0.75])\nIQR = Q3 - Q1\nlower_bound, upper_bound = Q1 - 1.5 * IQR, Q3 + 1.5 * IQR\ndf_cleaned = df[(df[\"price\"] >= lower_bound) & (df[\"price\"] <= upper_bound)].copy()\n\n# Feature engineering\ndf_cleaned[\"log_area\"] = np.log(df_cleaned[\"area\"])\nfeatures = [\"log_area\", \"bedrooms\", \"bathrooms\", \"stories\", \"parking\",\n            \"mainroad\", \"guestroom\", \"basement\", \"hotwaterheating\",\n            \"airconditioning\", \"prefarea\", \"furnishingstatus\"]\ntarget = df_cleaned[\"price\"]\n\nX = df_cleaned[features]\ny = target\n\n# Preprocessing\npreprocessor = ColumnTransformer(\n    transformers=[\n        (\"num\", StandardScaler(), [\"log_area\", \"bathrooms\", \"bedrooms\", \"stories\", \"parking\"]),\n        (\"cat\", OneHotEncoder(), [\"mainroad\", \"guestroom\", \"basement\", \"hotwaterheating\",\n                                  \"airconditioning\", \"prefarea\", \"furnishingstatus\"]),\n    ]\n)\n\n# Pipeline\npipeline = Pipeline(\n    steps=[\n        (\"preprocessor\", preprocessor),\n        (\"regressor\", XGBRegressor(objective=\"reg:squarederror\", random_state=42))\n    ]\n)\n\n# Hyperparameter tuning\nparam_grid = {\n    \"regressor__n_estimators\": [100, 200, 500],\n    \"regressor__learning_rate\": [0.01, 0.05, 0.1],\n    \"regressor__max_depth\": [3, 5, 7],\n    \"regressor__subsample\": [0.8, 1.0],\n    \"regressor__colsample_bytree\": [0.8, 1.0]\n}\n\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\ngrid_search = GridSearchCV(pipeline, param_grid, cv=kf, scoring=\"neg_mean_squared_error\", verbose=1)\ngrid_search.fit(X, y)\n\n# Best model\nbest_model = grid_search.best_estimator_\nprint(f\"Best hyperparameters: {grid_search.best_params_}\")\nprint(f\"Best score (neg MSE): {grid_search.best_score_}\")\n\n# Cross-validation evaluation\nmse_list, r2_list, mae_list = [], [], []\n\nfor fold, (train_idx, test_idx) in enumerate(kf.split(X, y)):\n    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n\n    model = best_model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n\n    mse_list.append(mean_squared_error(y_test, y_pred))\n    r2_list.append(r2_score(y_test, y_pred))\n    mae_list.append(mean_absolute_error(y_test, y_pred))\n\n    print(f\"Fold {fold + 1}: MSE={mse_list[-1]}, R²={r2_list[-1]}, MAE={mae_list[-1]}\")\n\n# Cross-validation results\nprint(\"\\nCross-Validation Results:\")\nprint(f\"Average MSE: {np.mean(mse_list):.2f}\")\nprint(f\"Average R²: {np.mean(r2_list):.4f}\")\nprint(f\"Average MAE: {np.mean(mae_list):.2f}\")\n\n# Get Predictions\n# Example new data (replace with your actual data)\nnew_data = pd.DataFrame({\n    \"log_area\": [8.5, 9.0, 7.8],\n    \"bedrooms\": [3, 4, 2],\n    \"bathrooms\": [2, 3, 1],\n    \"stories\": [2, 3, 1],\n    \"parking\": [1, 2, 0],\n    \"mainroad\": [\"yes\", \"yes\", \"no\"],\n    \"guestroom\": [\"no\", \"yes\", \"no\"],\n    \"basement\": [\"no\", \"yes\", \"no\"],\n    \"hotwaterheating\": [\"no\", \"no\", \"no\"],\n    \"airconditioning\": [\"yes\", \"yes\", \"no\"],\n    \"prefarea\": [\"yes\", \"no\", \"no\"],\n    \"furnishingstatus\": [\"furnished\", \"semi-furnished\", \"unfurnished\"]\n})\n\n# Get predictions\npredictions = best_model.predict(new_data)\n\n# Print predictions\nprint(\"\\nPredicted House Prices:\")\nprint(predictions)","metadata":{"_uuid":"634538ff-920f-4aa0-a7c4-92a9f9fb400d","_cell_guid":"da84317a-9a7f-44e8-849d-01595fee0367","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-02-22T13:20:33.286823Z","iopub.execute_input":"2025-02-22T13:20:33.287229Z","iopub.status.idle":"2025-02-22T13:22:05.303172Z","shell.execute_reply.started":"2025-02-22T13:20:33.287205Z","shell.execute_reply":"2025-02-22T13:22:05.302229Z"}},"outputs":[{"name":"stdout","text":"Fitting 5 folds for each of 108 candidates, totalling 540 fits\nBest hyperparameters: {'regressor__colsample_bytree': 0.8, 'regressor__learning_rate': 0.1, 'regressor__max_depth': 3, 'regressor__n_estimators': 100, 'regressor__subsample': 0.8}\nBest score (neg MSE): -845086862452.5889\nFold 1: MSE=1149840969015.694, R²=0.6679274665556774, MAE=803013.2759433963\nFold 2: MSE=1164599392120.7954, R²=0.5917222778453479, MAE=844065.1344339623\nFold 3: MSE=515956421858.6102, R²=0.6361916696987691, MAE=579082.766509434\nFold 4: MSE=750675662815.648, R²=0.6915832353459215, MAE=657300.9599056604\nFold 5: MSE=644361866452.1969, R²=0.7300459914435349, MAE=636941.7075471698\n\nCross-Validation Results:\nAverage MSE: 845086862452.59\nAverage R²: 0.6635\nAverage MAE: 704080.77\n\nPredicted House Prices:\n[6913160.5 7696461.  2568069.2]\n","output_type":"stream"}],"execution_count":3}]}